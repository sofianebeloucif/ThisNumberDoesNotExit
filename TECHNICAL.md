# ğŸ“˜ Technical Documentation

## Table of Contents

1. [Algorithm Overview](#algorithm-overview)
2. [Mathematical Foundation](#mathematical-foundation)
3. [Implementation Details](#implementation-details)
4. [Performance Analysis](#performance-analysis)
5. [Optimization Techniques](#optimization-techniques)

---

## ğŸ§  Algorithm Overview

### Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        INPUT: MNIST                          â”‚
â”‚                   60,000 images (28Ã—28)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 STEP 1: DIMENSIONALITY REDUCTION            â”‚
â”‚                          (PCA)                               â”‚
â”‚                                                              â”‚
â”‚  Input:  784 dimensions (28Ã—28 pixels)                      â”‚
â”‚  Output: 50 dimensions                                       â”‚
â”‚  Variance Retained: ~95%                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STEP 2: DENSITY ESTIMATION                      â”‚
â”‚                        (KDE)                                 â”‚
â”‚                                                              â”‚
â”‚  Kernel: Gaussian                                            â”‚
â”‚  Bandwidth: Auto-optimized (CV)                              â”‚
â”‚  Training Samples: 10,000 per KDE                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               STEP 3: SAMPLING + REJECTION                   â”‚
â”‚                                                              â”‚
â”‚  1. Sample from KDE                                          â”‚
â”‚  2. Calculate log-density                                    â”‚
â”‚  3. Accept if density â‰¥ threshold                            â”‚
â”‚  4. Repeat until n samples accepted                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               STEP 4: RECONSTRUCTION                         â”‚
â”‚                    (Inverse PCA)                             â”‚
â”‚                                                              â”‚
â”‚  Input:  50D latent vector                                   â”‚
â”‚  Output: 784D pixel values                                   â”‚
â”‚  Range:  [0, 1] (clipped)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                STEP 5: POST-PROCESSING                       â”‚
â”‚                  (Image Cleaning)                            â”‚
â”‚                                                              â”‚
â”‚  â€¢ Bilateral Denoising                                       â”‚
â”‚  â€¢ Thresholding                                              â”‚
â”‚  â€¢ Morphological Operations                                  â”‚
â”‚  â€¢ Small Component Removal                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                   OUTPUT: 28Ã—28 Image
```

---

## ğŸ“ Mathematical Foundation

### 1. Principal Component Analysis (PCA)

**Goal:** Reduce dimensionality while preserving variance.

**Formulation:**

Given data matrix $X \in \mathbb{R}^{n \times d}$ (n samples, d dimensions):

1. **Center the data:**
   $$\bar{X} = X - \mu$$
   where $\mu$ is the mean vector.

2. **Compute covariance matrix:**
   $$C = \frac{1}{n-1} \bar{X}^T \bar{X}$$

3. **Eigendecomposition:**
   $$C = V \Lambda V^T$$
   where $V$ are eigenvectors and $\Lambda$ are eigenvalues.

4. **Project to lower dimension:**
   $$Z = \bar{X} V_k$$
   where $V_k$ are the top-k eigenvectors.

5. **Reconstruct:**
   $$\hat{X} = Z V_k^T + \mu$$

**Variance Retained:**
$$\text{Variance Ratio} = \frac{\sum_{i=1}^{k} \lambda_i}{\sum_{i=1}^{d} \lambda_i}$$

For our case: $k=50$, variance â‰ˆ 85%

---

### 2. Kernel Density Estimation (KDE)

**Goal:** Estimate the probability density function of data.

**Formulation:**

$$\hat{f}_h(x) = \frac{1}{nh} \sum_{i=1}^{n} K\left(\frac{x - x_i}{h}\right)$$

Where:
- $K$ is the **kernel function** (we use Gaussian)
- $h$ is the **bandwidth** (smoothing parameter)
- $n$ is the number of training samples
- $x_i$ are the training data points

**Gaussian Kernel:**
$$K(u) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}u^2}$$

**In multivariate case (our 50D space):**
$$\hat{f}_H(x) = \frac{1}{n} \sum_{i=1}^{n} K_H(x - x_i)$$

$$K_H(x) = |H|^{-1/2} K(H^{-1/2}x)$$

where $H$ is the bandwidth matrix (we use $H = h^2 I$).

---

### 3. Bandwidth Selection

**Problem:** Choose optimal $h$ to balance bias and variance.

**Method:** Grid Search with Cross-Validation

1. **Candidate bandwidths:**
   $$h \in \{0.5, 0.7, 0.9, ..., 2.3, 2.5\}$$

2. **Scoring metric:** Log-likelihood
   $$\mathcal{L}(h) = \sum_{i=1}^{n} \log \hat{f}_h(x_i^{\text{test}})$$

3. **5-fold cross-validation:**
   - Split data into 5 folds
   - For each fold: train on 4, test on 1
   - Average scores across folds

4. **Select:**
   $$h^* = \arg\max_{h} \mathcal{L}(h)$$

**Result:** Typically $h^* \in [1.0, 1.5]$ for MNIST.

---

### 4. Rejection Sampling

**Goal:** Improve sample quality by filtering.

**Algorithm:**

```
Input: Target distribution p(x), Proposal distribution q(x)
       Constant M such that p(x) â‰¤ MÂ·q(x) for all x

1. Sample x ~ q(x)                    [From KDE]
2. Sample u ~ Uniform(0, 1)
3. Accept x if u â‰¤ p(x) / (MÂ·q(x))   [If log-density â‰¥ threshold]
4. Else reject and go to step 1
```

**In our case:**
- $q(x)$ = KDE distribution
- $p(x)$ = Truncated KDE (only high-density regions)
- **Threshold** = percentile of training data log-densities

**Acceptance Rate:**
$$\text{Rate} = P(\text{accept}) = \frac{1}{M}$$

For P25: Rate â‰ˆ 65%  
For P50: Rate â‰ˆ 45%

---

### 5. Image Cleaning

**a) Bilateral Denoising**

Preserves edges while smoothing:

$$I_{\text{filtered}}(x) = \frac{1}{W_p} \sum_{x_i \in \Omega} I(x_i) f_r(\|I(x_i) - I(x)\|) g_s(\|x_i - x\|)$$

Where:
- $f_r$ is range kernel (intensity similarity)
- $g_s$ is spatial kernel (distance)
- $W_p$ is normalization factor

**b) Thresholding**

$$I_{\text{thresh}}(x) = \begin{cases}
I(x) & \text{if } I(x) \geq \tau \\
0 & \text{otherwise}
\end{cases}$$

Typical $\tau \in [0.2, 0.3]$

**c) Morphological Closing**

$$I \bullet B = (I \oplus B) \ominus B$$

Where:
- $\oplus$ is dilation
- $\ominus$ is erosion
- $B$ is structuring element

Fills small holes and connects nearby regions.

**d) Connected Component Analysis**

Remove components with area $< \alpha$:
$$I_{\text{clean}} = I \setminus \{C : |C| < \alpha\}$$

Typical $\alpha \in [8, 12]$ pixels.

---

## ğŸ’» Implementation Details

### Class Architecture

```python
class GlobalGenerator:
    """Single KDE for all digits"""
    
    def __init__(self, n_components=50, bandwidth=1.2):
        self.pca = PCA(n_components)
        self.kde = KernelDensity(bandwidth, kernel='gaussian')
    
    def fit(self, X):
        X_pca = self.pca.fit_transform(X)
        self.kde.fit(X_pca)
    
    def generate(self, n, use_rejection=True, clean=True):
        if use_rejection:
            z = self._rejection_sampling(n, threshold)
        else:
            z = self.kde.sample(n)
        
        X_recon = self.pca.inverse_transform(z)
        X_recon = np.clip(X_recon, 0, 1)
        
        if clean:
            X_recon = ImageCleaner.clean(X_recon)
        
        return X_recon.reshape(-1, 28, 28)

class ConditionalGenerator:
    """Separate KDE per digit"""
    
    def __init__(self, n_components=50, bandwidth=1.2):
        self.pca = PCA(n_components)
        self.kde_models = {}  # Dict: digit â†’ KDE
    
    def fit(self, X, y):
        X_pca = self.pca.fit_transform(X)
        
        for digit in range(10):
            mask = (y == digit)
            X_digit = X_pca[mask]
            
            kde = KernelDensity(bandwidth, kernel='gaussian')
            kde.fit(X_digit)
            self.kde_models[digit] = kde
```

### Memory Optimization

**Problem:** Storing 10 KDE models can be memory-intensive.

**Solution:** Share PCA across all KDEs

```
Global:       PCA (5MB) + 1 KDE (0.5MB) â‰ˆ 5.5 MB
Conditional:  PCA (5MB) + 10 KDE (10Ã—0.5MB) â‰ˆ 10 MB
```

**Further optimization:**
- Sub-sample training data for KDE (use 5,000 samples instead of 60,000)
- Use ball tree for faster queries
- Quantize bandwidth to float16

---

## ğŸ“Š Performance Analysis

### Time Complexity

| Operation | Complexity | Typical Time |
|-----------|-----------|--------------|
| **PCA Fit** | O(dÂ²n + dÂ³) | ~2s |
| **PCA Transform** | O(ndk) | ~10ms |
| **KDE Fit** | O(n) | ~0.5s |
| **KDE Sample** | O(n) | ~1ms per sample |
| **Rejection Loop** | O(Î±n) | ~10ms (Î±â‰ˆ1.5) |
| **Image Cleaning** | O(hw log hw) | ~2ms per image |

Where:
- n = number of samples
- d = original dimensions (784)
- k = reduced dimensions (50)
- h, w = image dimensions (28)
- Î± = rejection factor (1/acceptance_rate)

### Space Complexity

| Component | Size | Notes |
|-----------|------|-------|
| PCA mean | 784 floats â‰ˆ 3KB | |
| PCA components | 50Ã—784 floats â‰ˆ 150KB | |
| KDE samples | 10,000Ã—50 floats â‰ˆ 2MB | Per KDE |
| **Global Total** | **~4.4 MB** | Actual measured size |
| **Conditional Total** | **~20.8 MB** | 10 KDEs, actual measured |

---

## ğŸ”§ Optimization Techniques

### 1. Bandwidth Optimization

**Naive approach:** Try all values, $O(|H| \cdot n \cdot k)$

**Optimized:**
```python
from sklearn.model_selection import GridSearchCV

grid = GridSearchCV(
    KernelDensity(kernel='gaussian'),
    {'bandwidth': np.linspace(0.5, 2.5, 10)},
    cv=5,
    n_jobs=-1  # Parallel
)
grid.fit(X_train_pca)
```

**Speedup:** ~5Ã— (parallelization)

### 2. Rejection Sampling Batching

**Naive:**
```python
while len(accepted) < n:
    sample = kde.sample(1)
    if accept(sample):
        accepted.append(sample)
```

**Optimized:**
```python
while len(accepted) < n:
    batch = kde.sample(n * 10)  # Oversample
    densities = kde.score_samples(batch)
    mask = densities >= threshold
    accepted.extend(batch[mask])
```

**Speedup:** ~10Ã— (vectorization)

### 3. Image Cleaning Pipeline

**Avoid redundant computations:**
```python
# Bad: Process each image separately
for img in images:
    img = denoise(img)
    img = threshold(img)
    img = clean_components(img)

# Good: Batch operations
images = denoise_batch(images)
images = threshold_batch(images)
images = clean_components_batch(images)
```

---

## ğŸ“š References

1. **PCA:** Pearson, K. (1901). "On Lines and Planes of Closest Fit to Systems of Points in Space"
2. **KDE:** Rosenblatt, M. (1956). "Remarks on Some Nonparametric Estimates of a Density Function"
3. **Rejection Sampling:** Von Neumann, J. (1951). "Various Techniques Used in Connection with Random Digits"
4. **MNIST:** LeCun, Y. et al. (1998). "Gradient-based learning applied to document recognition"

---

## ğŸ’¡ Tips & Tricks

### For Better Quality
1. Use stricter rejection (P25 â†’ P50)
2. Enable aggressive cleaning
3. Combine all three above

### For Faster Generation
1. Disable rejection sampling
2. Use light cleaning
3. Reduce PCA components (50 â†’ 25)
4. Cache frequently used samples

### For Smaller Models
1. Reduce PCA components
2. Sub-sample KDE training data
3. Use ball tree for KDE
4. Quantize to float16

---

<div align="center">

**[â† Back to Main README](README.md)**

</div>